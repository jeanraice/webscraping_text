{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfP+gLYp5f+R5ha7976N6w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanraice/webscraping_text/blob/main/Webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import the useful Library"
      ],
      "metadata": {
        "id": "lE9wUEeqfebd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p02vzd9HcoPH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13l28f05q2yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a csv file and save the data we scrap on it \n",
        "\n",
        "> Bloc en retrait\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GCIFvgrcf2Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "web_site = requests.get('http://coreyms.com').text\n",
        "\n",
        "soup = BeautifulSoup(web_site, 'lxml')\n",
        "\n",
        "csv_file = open('scraping.csv', 'w')\n",
        "\n",
        "csv_scraping = csv.writer(csv_file)\n",
        "csv_scraping.writerow(['headline', 'summary', 'video_link'])\n",
        "\n",
        "for article in soup.find_all('article'):\n",
        "    headline = article.h2.a.text\n",
        "    #print(headline)\n",
        "\n",
        "    summary = article.find('div', class_='entry-content').p.text\n",
        "    #print(summary)\n",
        "\n",
        "    try:\n",
        "        vid_src = article.find('iframe', class_='youtube-player')['src']\n",
        "\n",
        "        vid_id = vid_src.split('/')[4] #split the text. The criterea of split is '/' and take the 5th element\n",
        "        vid_id = vid_id.split('?')[0] #split the text. The criterea of split is '?' and take the 1st element\n",
        "\n",
        "        yt_link = f'https://youtube.com/watch?v={vid_id}' #create the youtube real like \n",
        "    except Exception as e:\n",
        "        yt_link = 'Link does not exist'\n",
        "\n",
        "    #print(yt_link)\n",
        "\n",
        "    #print()\n",
        "\n",
        "    csv_scraping.writerow([headline, summary, yt_link])\n",
        "\n",
        "csv_file.close()\n"
      ],
      "metadata": {
        "id": "sa0GM5aLf_tW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v37ABWvoglQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}